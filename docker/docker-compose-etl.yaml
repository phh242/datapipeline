name: speechcommands-etl

volumes:
  speechcommands:

services:
  extract-data:
    container_name: etl_extract_data
    image: python:3.11
    user: root
    volumes:
      - speechcommands:/data
    working_dir: /data
    command:
      - bash
      - -c
      - |
        set -e

        echo "Downloading main and test datasets..."
        curl -L http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz \
          -o speech_commands_v0.02.tar.gz
        curl -L http://download.tensorflow.org/data/speech_commands_test_set_v0.02.tar.gz \
          -o speech_commands_test_set_v0.02.tar.gz

        echo "Unzipping main and test datasets..."
        mkdir -p speech_commands_v0.02
        tar -xzf speech_commands_v0.02.tar.gz -C speech_commands_v0.02
        rm speech_commands_v0.02.tar.gz

        mkdir -p speech_commands_test_set_v0.02
        tar -xzf speech_commands_test_set_v0.02.tar.gz -C speech_commands_test_set_v0.02
        rm speech_commands_test_set_v0.02.tar.gz

        echo "Listing contents of /data after extract stage:"
        ls -l /data

  process-data:
    container_name: etl_process_data
    image: python:3.11
    volumes:
      - speechcommands:/data
    working_dir: /data
    command:
      - bash
      - -c
      - |
        set -e

        apt-get update && apt-get install -y ffmpeg libsox-dev
        pip install --no-cache-dir numpy torchaudio pydub

        python3 -c '
        import os
        import random
        import numpy as np
        import torchaudio
        from pydub import AudioSegment
        from pathlib import Path

        def normalize(audio, target_dBFS=-1.0):
            return audio.apply_gain(target_dBFS - audio.max_dBFS)

        def overlay(speech, background):
            # adjust background audio volume
            background = background - 8
            # overlay background audio to speech audio
            return normalize(speech.overlay(background), target_dBFS=-1.0)

        def normalize_overlay(dataset_dir, subdirs):
            normalized_dataset_dir = dataset_dir + "_processed"
            for subdir in subdirs:
                print(subdir)
                dir_path = os.path.join(dataset_dir, subdir)
                normalized_dir_path = os.path.join(normalized_dataset_dir, subdir)
                os.makedirs(normalized_dir_path, exist_ok=True)

                # randomly select half of the files
                selected_files = [entry.name for entry in os.scandir(dir_path) if entry.is_file()]
                random.shuffle(selected_files)
                half_files = selected_files[:len(selected_files) // 2]

                for file in half_files:
                    if not file.endswith(".wav"):
                        continue

                    file_path = os.path.join(dir_path, file)
                    normalized_file_path = os.path.join(normalized_dir_path, file)
                    # normalize background audio files to target_dBFS=-3.0
                    audio = normalize(AudioSegment.from_file(file_path), target_dBFS=-3.0)
                    audio.export(normalized_file_path, format="wav")

                    if subdir.startswith("_silence_"):
                        continue

                    for background in os.listdir(background_dir):
                        background_path = os.path.join(background_dir, background)
                        # overlay two audio files and export the mixed file
                        mixed = overlay(audio, AudioSegment.from_file(background_path))
                        mixed_name = Path(Path(normalized_file_path).stem).stem + "_" + background
                        mixed_path = os.path.join(normalized_dir_path, mixed_name)
                        mixed.export(mixed_path, format="wav")

        # define path
        base_dir = "/data"
        dataset_base_dir = os.path.join(base_dir, "speech_commands_v0.02")
        background_subdirs = ["_background_noise_"]

        # normalize background audio files
        for subdir in background_subdirs:
            dir_path = dir_path = os.path.join(dataset_base_dir, subdir)
            normalized_dir_path = os.path.join(base_dir, subdir) + "normalized"
            os.makedirs(normalized_dir_path, exist_ok=True)
            for file in os.listdir(dir_path):
                if not file.endswith(".wav"):
                    continue
                file_path = os.path.join(dir_path, file)
                normalized_file_path = os.path.join(normalized_dir_path, file)
                # normalize background audio files to target_dBFS=-6.0
                audio = normalize(AudioSegment.from_file(file_path), target_dBFS=-6.0)
                audio.export(normalized_file_path, format="wav")

        background_dir = base_dir + "/_background_noise_normalized"

        # normalize and overlay main with background audio files
        base_subdirs = ["backward", "bed", "bird", "cat", "dog", "down", "eight", "five", 
                        "follow", "forward", "four", "go", "happy", "house", "learn", "left", 
                        "marvin", "nine", "no", "off", "on", "one", "right", "seven", "sheila", 
                        "six", "stop", "three", "tree", "two", "up", "visual", "wow", "yes", "zero"]
        # print("Normalizing and mixing main dataset...")
        # normalize_overlay(dataset_base_dir, base_subdirs)
        # print("Main dataset process complete.")

        # normalize and overlay test with background audio files
        dataset_test_dir = os.path.join(base_dir, "speech_commands_test_set_v0.02")
        test_subdirs = ["_silence_", "_unknown_", "down", "go", "left", "no", "off",
                        "on", "right", "stop", "up", "yes"]
        print("Normalizing and mixing test dataset...")
        normalize_overlay(dataset_test_dir, test_subdirs)
        print("Test dataset process complete.")

        def melspectrogram(file_path, mel_file_path):
            waveform, sample_rate = torchaudio.load(file_path, normalize=True)
            transform = torchaudio.transforms.MelSpectrogram(sample_rate, n_fft=512, n_mels=80)
            mel_specgram = transform(waveform)
            np.save(mel_file_path, mel_specgram.numpy())

        def generate_mel(dataset_dir, subdirs):
            mel_dataset_dir = dataset_dir + "_processed_mel"
            for subdir in subdirs:
                print(subdir)
                dir_path = os.path.join(dataset_dir + "_processed", subdir)
                mel_dir_path = os.path.join(mel_dataset_dir, subdir)
                os.makedirs(mel_dir_path, exist_ok=True)

                for file in os.listdir(dir_path):
                    if not file.endswith(".wav"):
                        continue

                    file_path = os.path.join(dir_path, file)
                    mal_file_name = Path(file).stem + ".npy"
                    mel_file_path = os.path.join(mel_dir_path, mal_file_name)

                    # generate mel spectrogram for wav file and save
                    melspectrogram(file_path, mel_file_path)

        print("Generating mel spectrogram for main dataset...")
        generate_mel(dataset_base_dir, base_subdirs)
        print("Generating complete.")

        print("Generating mel spectrogram for test dataset...")
        generate_mel(dataset_test_dir, test_subdirs)
        print("Generating complete.")
        '
        
        echo "Listing contents of /data after transform stage:"
        ls -l /data

  transform-data:
    container_name: etl_transform_data
    image: python:3.11
    volumes:
      - speechcommands:/data
    working_dir: /data
    command:
      - bash
      - -c
      - |
        set -e

        python3 -c '
        import os
        import re
        import hashlib
        import shutil
        from pathlib import Path

        MAX_NUM_WAVS_PER_CLASS = 2**27 - 1  # ~134M

        # determines which data partition the file should belong to
        def which_set(filename, validation_percentage, testing_percentage):
            base_name = os.path.basename(filename)
            # We want to ignore anything after '_nohash_' in the file name when
            # deciding which set to put a wav in, so the data set creator has a way of
            # grouping wavs that are close variations of each other.
            hash_name = re.sub(r"_nohash_.*", "", base_name)
            # This looks a bit magical, but we need to decide whether this file should
            # go into the training, testing, or validation sets, and we want to keep
            # existing files in the same set even if more files are subsequently
            # added.
            # To do that, we need a stable way of deciding based on just the file name
            # itself, so we do a hash of that and then use that to generate a
            # probability value that we use to assign it.
            hash_name_hashed = hashlib.sha1(hash_name.encode("utf-8")).hexdigest()
            #   hash_name_hashed = hashlib.sha1(hash_name).hexdigest()

            percentage_hash = ((int(hash_name_hashed, 16) %
                                (MAX_NUM_WAVS_PER_CLASS + 1)) *
                                (100.0 / MAX_NUM_WAVS_PER_CLASS))
            if percentage_hash < validation_percentage:
                result = "validation"
            elif percentage_hash < (testing_percentage + validation_percentage):
                result = "evaluation"
                # result = 'testing'
            else:
                result = "training"
            return result

        base_dir = "/data"
        dataset_base_dir = os.path.join(base_dir, "speech_commands_v0.02_processed")
        dataset_mel_dir = os.path.join(base_dir, "speech_commands_v0.02_processed_mel")
        dataset_subdirs = ["training", "validation", "evaluation"]
        base_subdirs = ["backward", "bed", "bird", "cat", "dog", "down", "eight", "five", 
                        "follow", "forward", "four", "go", "happy", "house", "learn", "left", 
                        "marvin", "nine", "no", "off", "on", "one", "right", "seven", "sheila", 
                        "six", "stop", "three", "tree", "two", "up", "visual", "wow", "yes", "zero"]

        for subdir in base_subdirs:
            print(subdir)
            dir_path = os.path.join(dataset_base_dir, subdir)
            mel_dir_path = os.path.join(dataset_mel_dir, subdir)

            if not os.path.exists(dir_path):
                continue

            for file in os.listdir(dir_path):  
                mel_file = Path(file).stem + ".npy"
                if not os.path.exists(os.path.join(mel_dir_path, mel_file)):
                    continue

                set = which_set(file, 10, 10)
                # move processed wav file into according set
                set_dir = os.path.join(dataset_base_dir, set, subdir)
                os.makedirs(set_dir, exist_ok=True)
                shutil.move(os.path.join(dir_path, file), os.path.join(set_dir, file))

                # mode the mel spectrogram into the same set
                mel_set_dir = os.path.join(dataset_mel_dir, set, subdir)
                os.makedirs(mel_set_dir, exist_ok=True)        
                shutil.move(os.path.join(mel_dir_path, mel_file), os.path.join(mel_set_dir, mel_file))
            
            shutil.rmtree(dir_path)
            shutil.rmtree(mel_dir_path)
          '

          echo "Listing contents of /data/speech_commands_v0.02_processed after transform stage:"
          ls -l /data/speech_commands_v0.02_processed

          # echo "Listing contents of /data/speech_commands_v0.02_processed_mel after transform stage:"
          # ls -l /data/speech_commands_v0.02_processed_mel

  load-data:
    container_name: etl_load_data
    image: rclone/rclone:latest
    volumes:
      - speechcommands:/data
      - ~/.config/rclone/rclone.conf:/root/.config/rclone/rclone.conf:ro
    entrypoint: /bin/sh
    command:
      - -c
      - |
        if [ -z "$RCLONE_CONTAINER" ]; then
          echo "ERROR: RCLONE_CONTAINER is not set"
          exit 1
        fi
        # echo "Cleaning up existing contents of container..."
        # rclone delete chi_tacc:$RCLONE_CONTAINER --rmdirs || true

        rclone copy /data/speech_commands_test* chi_tacc:$RCLONE_CONTAINER \
        --progress \
        --transfers=32 \
        --checkers=16 \
        --multi-thread-streams=4 \
        --fast-list

        echo "Listing directories in container after load stage:"
        rclone lsd chi_tacc:$RCLONE_CONTAINER
